{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e888e3ec",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "The goal of `simpple` is to have a tiny [probablistic programming](https://en.wikipedia.org/wiki/Probabilistic_programming) language compatible with some of the most common sampling libraries.\n",
    "That way, it is easy to write a model (including all priors) for `emcee`, and then run it with `nautilus`, for example.\n",
    "\n",
    "In this tutorial, we will do a short demo by sampling a 3D normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpple\n",
    "print(simpple.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453708c",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "The two key components we need to specify in a Bayesian model are the prior distribution and the likelihood function.\n",
    "Pretty much all sampling libraries require these two components, either separately for nested sampling, or combined in a posterior for MCMC.\n",
    "\n",
    "In `simpple`, the prior is specified as a dictionary of `Distribution` objects.\n",
    "Most common distributions are already implemented in scipy, so the recommended approach, inspired by [nautilus](https://nautilus-sampler.readthedocs.io/en/latest/guides/priors.html), is to simply wrap scipy distributions whenever possible.\n",
    "\n",
    "```{note}\n",
    "One specificity of scipy distributions compared to most other libarries is that the `uniform` distribution has parameters \"lower\" and \"width\" instead of \"lower\" and \"upper\".\n",
    "\n",
    "For example, `uniform(-5, 10)` is a uniform distribution between -5 and 5.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, uniform\n",
    "\n",
    "from simpple.distributions import ScipyDistribution\n",
    "\n",
    "parameters = {\n",
    "    \"x1\": ScipyDistribution(uniform, -5, 10),\n",
    "    \"x2\": ScipyDistribution(uniform, 0, 10),\n",
    "    \"x3\": ScipyDistribution(norm(0, 10)),\n",
    "}\n",
    "print(\"Priors:\")\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bfa29",
   "metadata": {},
   "source": [
    "Next, we need a log-likelihood function that takes a dictionary of parameters and computes the likelihood.\n",
    "In this simple 3D Gaussian example, we compare each parameter directly with a \"data point\" (`mean`), and the noise is correlated (defined by `cov`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00829aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def log_likelihood(params):\n",
    "    \"\"\"Log-likelihood function for a 3D normal distribution.\"\"\"\n",
    "    p = [params[\"x1\"], params[\"x2\"], params[\"x3\"]]\n",
    "    mean = [0.0, 3.0, 2.0]\n",
    "    cov = [[1, 0.5, 0], [0.5, 1, 0], [0, 0, 1.0]]\n",
    "    return multivariate_normal.logpdf(p, mean=mean, cov=cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1b610",
   "metadata": {},
   "source": [
    "It is now time to create our `simpple.Model` object.\n",
    "The model needs to know what our priors and likelihood are.\n",
    "It will then wrap them to provide:\n",
    "\n",
    "- `log_prior(parameters)`: the prior distribution given a dictionary or an array of parameters\n",
    "- `log_prob(parameters)`: the posterior distribution given a dictionary or an array of parameters\n",
    "- `log_likelihood(parameters)`: a wrapper around our log-likelihood above to make it work with arrays and dictionaries\n",
    "- `prior_transform(u)`: a prior transform from a unit hypercube to our parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc974684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpple.model import Model\n",
    "\n",
    "model = Model(parameters, log_likelihood)\n",
    "\n",
    "print(model)\n",
    "test_point = [0, 1, 0]\n",
    "print(\"Log-Prior\", model.log_prior(test_point))\n",
    "print(\"Log-Prior out of bounds\", model.log_prior([-10, 3, 0]))\n",
    "print(\"Log-likelihood\", model.log_likelihood(test_point))\n",
    "print(\"Log-posterior\", model.log_prob(test_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22762603",
   "metadata": {},
   "source": [
    "## Prior Checks\n",
    "\n",
    "A good thing to do before fitting any model is to check the prior.\n",
    "To sample directly from the prior, we can either pass a uniform distribution through the `prior_transform` function or sample the `log_prior()` function of our model with `emcee`.\n",
    "\n",
    "Here, we will test both approaches.\n",
    "In practice, it's probably a good idea to test your prior transform if you plan on using nested sampling and your log-prior if you plan on using MCMC.\n",
    "\n",
    "### Via prior transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "u = np.random.uniform(0, 1, size=(3, 100_000))\n",
    "prior_samples = model.prior_transform(u)\n",
    "\n",
    "fig = corner.corner(prior_samples.T, labels=model.keys())\n",
    "fig.suptitle(\"Prior samples via prior transform\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358a817",
   "metadata": {},
   "source": [
    "### Via emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd04fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers=100,\n",
    "    ndim=3,\n",
    "    log_prob_fn=model.log_prior,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.array(test_point) + 1e-4 * rng.standard_normal(size=(100, 3))\n",
    "sampler.run_mcmc(p0, 1000, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe290b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = sampler.get_chain()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1)\n",
    "for i in range(3):\n",
    "    axs[i].plot(chains[:, :, i], \"k-\", alpha=0.1)\n",
    "    axs[i].set_ylabel(model.keys()[i])\n",
    "axs[-1].set_xlabel(\"Steps\")\n",
    "axs[0].set_title(\"Prior chains from emcee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chains = sampler.get_chain(discard=200, flat=True)\n",
    "fig = corner.corner(flat_chains, labels=model.keys())\n",
    "fig.suptitle(\"Prior samples via emcee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68958021",
   "metadata": {},
   "source": [
    "## Sampling with `emcee`\n",
    "\n",
    "We can now sample the posterior with emcee. We can basically copy the previous subsection, but replace `log_prior` with `log_prob` in our sampler definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers=100,\n",
    "    ndim=3,\n",
    "    log_prob_fn=model.log_prob,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.array(test_point) + 1e-4 * rng.standard_normal(size=(100, 3))\n",
    "sampler.run_mcmc(p0, 1000, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = sampler.get_chain()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1)\n",
    "for i in range(3):\n",
    "    axs[i].plot(chains[:, :, i], \"k-\", alpha=0.1)\n",
    "    axs[i].set_ylabel(model.keys()[i])\n",
    "axs[-1].set_xlabel(\"Steps\")\n",
    "axs[0].set_title(\"Prior chains from emcee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73611cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chains = sampler.get_chain(discard=200, flat=True)\n",
    "fig = corner.corner(flat_chains, labels=model.keys())\n",
    "fig.suptitle(\"Prior samples via emcee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0e8ca",
   "metadata": {},
   "source": [
    "## Sampling with `ultranest`\n",
    "\n",
    "Next, let us try and sample the model with Ultranest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultranest\n",
    "\n",
    "sampler = ultranest.ReactiveNestedSampler(\n",
    "    model.keys(), model.log_likelihood, model.prior_transform\n",
    ")\n",
    "\n",
    "result = sampler.run(show_status=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336aa242",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.plot_corner()\n",
    "plt.show()\n",
    "sampler.plot_run()\n",
    "plt.show()\n",
    "sampler.plot_trace()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c5817",
   "metadata": {},
   "source": [
    "## Sampling with `nautilus`\n",
    "\n",
    "The Nautilus sampler uses a slightly different format for the prior specification.\n",
    "`simpple` can also interface with nautilus using `Model.nautilus_prior()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086437b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nautilus import Sampler\n",
    "\n",
    "sampler = Sampler(model.nautilus_prior(), model.log_likelihood, n_live=1000)\n",
    "sampler.run(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc33131",
   "metadata": {},
   "outputs": [],
   "source": [
    "points, log_w, log_l = sampler.posterior()\n",
    "corner.corner(\n",
    "    points,\n",
    "    weights=np.exp(log_w),\n",
    "    labels=model.keys(),\n",
    "    plot_datapoints=False,\n",
    "    range=np.repeat(0.999, len(model.parameters)),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26634204",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Now that we have explored the posterior with several samplers, we can compare the resulting distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "\n",
    "hist_kwargs = dict(density=True)\n",
    "\n",
    "fig = corner.corner(\n",
    "    points,\n",
    "    weights=np.exp(log_w),\n",
    "    labels=model.keys(),\n",
    "    color=\"purple\",\n",
    "    hist_kwargs=hist_kwargs,\n",
    "    plot_datapoints=False,\n",
    "    range=np.repeat(0.999, len(model.parameters)),\n",
    ")\n",
    "data = np.array(result[\"weighted_samples\"][\"points\"])\n",
    "weights = np.array(result[\"weighted_samples\"][\"weights\"])\n",
    "corner.corner(\n",
    "    data,\n",
    "    weights=weights,\n",
    "    color=\"red\",\n",
    "    hist_kwargs=hist_kwargs,\n",
    "    plot_datapoints=False,\n",
    "    range=np.repeat(0.999, len(model.parameters)),\n",
    "    fig=fig,\n",
    ")\n",
    "corner.corner(\n",
    "    flat_chains,\n",
    "    weights=np.ones(flat_chains.shape[0]),\n",
    "    color=\"k\",\n",
    "    hist_kwargs=hist_kwargs,\n",
    "    plot_datapoints=False,\n",
    "    fig=fig,\n",
    ")\n",
    "\n",
    "\n",
    "nautilus_patch = patches.Patch(color=\"purple\", label=\"Nautilus\")\n",
    "ultranest_patch = patches.Patch(color=\"red\", label=\"Ultranest\")\n",
    "emcee_patch = patches.Patch(color=\"k\", label=\"Emcee\")\n",
    "\n",
    "fig.legend(\n",
    "    handles=[nautilus_patch, ultranest_patch, emcee_patch],\n",
    "    loc=\"upper right\",  # You can also use 'upper left', 'lower right', etc.\n",
    "    bbox_to_anchor=(0.98, 0.98),\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
