{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06526f83",
   "metadata": {},
   "source": [
    "# Fitting a line to data\n",
    "\n",
    "The [_Getting started_ tutorial](./getting-started.ipynb) shows how to sample a 3D gaussian with `simpple`.\n",
    "In this tutorial, we will build on this to demonstrate a more realistic scenario where we fit a line to data.\n",
    "\n",
    "## Simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "x = np.sort(10 * rng.random(100))\n",
    "m_true = 1.338\n",
    "b_true = -0.45\n",
    "truths = {\"m\": m_true, \"b\": b_true, \"sigma\": None}\n",
    "y_true = m_true * x + b_true\n",
    "yerr = 0.1 + 0.5 * rng.random(x.size)\n",
    "y = y_true + 2 * yerr * rng.normal(size=x.size)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(x, y_true, label=\"True signal\")\n",
    "ax.errorbar(x, y, yerr=yerr, fmt=\"k.\", capsize=2, label=\"Simulated data\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3d017",
   "metadata": {},
   "source": [
    "## Linear model\n",
    "\n",
    "To define our linear model with `simpple`, we could direclty write the log-likelihood function and calculate the model inside of it.\n",
    "A slightly more granular approach is to first define our \"forward model\" function and then call that function from the log-likelihood.\n",
    "Notice that we add an extra parameter to inflate the uncertainties in the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(p, x):\n",
    "    return p[\"m\"] * x + p[\"b\"]\n",
    "\n",
    "def log_likelihood(p, x, y, yerr):\n",
    "    ymod = linear_model(p, x)\n",
    "    var = yerr**2 + p[\"sigma\"]**2\n",
    "    return - 0.5 * np.sum(np.log(2 * np.pi * var) + (y - ymod)**2 / var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fec6e",
   "metadata": {},
   "source": [
    "`simpple` has a dedicated class for models that consist of a forward model called in the likelihood: `simpple.model.ForwardModel`.\n",
    "Using this class instead of the base `Model` has two main advantages: we can sample predictions from the prior with `get_prior_pred()`\n",
    "and the forward model, accessible through `ForwardModel.forward()` is wrapped to accept either a dictionary or an array.\n",
    "Let us define such a model. We will use uniform priors on the linear parameters and a log-uniform on `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "from simpple.distributions import ScipyDistribution\n",
    "from simpple.model import ForwardModel\n",
    "\n",
    "parameters = {\n",
    "    \"m\": ScipyDistribution(uniform(-10, 20)),\n",
    "    \"b\": ScipyDistribution(uniform(-10, 20)),\n",
    "    \"sigma\": ScipyDistribution(loguniform(1e-5, 100)),\n",
    "}\n",
    "\n",
    "model = ForwardModel(parameters, log_likelihood, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb7b72",
   "metadata": {},
   "source": [
    "First, we can check that the model works as expected on a single test point.\n",
    "Note that all arguments required by the likelihood are also required by the log-posterior (`log_prob()`)\n",
    "They will then be passed to the likelihood internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a31285",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = {\"m\": 1.0, \"b\": 0, \"sigma\": 1.0}\n",
    "print(\"Log prior\", model.log_prior(test_point))\n",
    "print(\"Log likelihood\", model.log_likelihood(test_point, x, y, yerr))\n",
    "print(\"Log probability\", model.log_prob(test_point, x, y, yerr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ff640",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(x, y_true, label=\"True signal\")\n",
    "ax.plot(x, model.forward(test_point, x), label=\"Test model\")\n",
    "ax.errorbar(x, y, yerr=yerr, fmt=\"k.\", capsize=2, label=\"Simulated data\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c37e36",
   "metadata": {},
   "source": [
    "## Prior checks\n",
    "\n",
    "First, we can do a prior check.\n",
    "We will look at the joint prior on parameters to make sure that we did not make any mistake in our specification.\n",
    "\n",
    "**Note**: We use [corner](https://corner.readthedocs.io/en/latest/) to display the posterior.\n",
    "You will need to install it along with [arviz](https://python.arviz.org/en/stable/) to display the prior and posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7394b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "n_prior = 1000\n",
    "prior_samples = model.get_prior_samples(n_prior)\n",
    "\n",
    "fig = corner.corner(prior_samples)\n",
    "fig.suptitle(\"Prior samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a9f9d8",
   "metadata": {},
   "source": [
    "We can also look at forward model predictions for prior samples.\n",
    "This gives a good idea of whether the parameter space covers all the data and yields reasonable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pred = 100\n",
    "ypreds = model.get_prior_pred(n_pred, x)\n",
    "ax = plt.gca()\n",
    "for i, ypred in enumerate(ypreds):\n",
    "    ax.plot(x, ypred, \"C1-\", label=\"Prior samples\" if i == 0 else None, alpha=0.1)\n",
    "ax.plot(x, y_true, label=\"True signal\")\n",
    "ax.errorbar(x, y, yerr=yerr, fmt=\"k.\", capsize=2, label=\"Simulated data\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_title(\"Prior predictive samples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61cd0ee",
   "metadata": {},
   "source": [
    "## Posterior sampling\n",
    "\n",
    "To sample the posterior distribution, let us use the [`zeus-mcmc`](https://zeus-mcmc.readthedocs.io/) package.\n",
    "Feel free to replace this with your favorite MCMC or nested sampling library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeus\n",
    "\n",
    "nwalkers = 100\n",
    "nsteps = 1000\n",
    "ndim = len(model.keys())\n",
    "start = np.array([0.0, 0.0, 10.0]) + rng.standard_normal(size=(nwalkers, ndim))\n",
    "sampler = zeus.EnsembleSampler(nwalkers, ndim, model.log_prob, args=(x, y, yerr))\n",
    "sampler.run_mcmc(start, nsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b90fb",
   "metadata": {},
   "source": [
    "## Posterior distribution and predictions\n",
    "\n",
    "Just like we did for the prior, we can now visualize the posterior and some predictive samples drawn from the posterior.\n",
    "\n",
    "First, let us look at the chains to make sure they are well-behaved.\n",
    "It is quite common to show the chains in plot like the one below, and they are somewhat cumbersone to code.\n",
    "For that reason, `simpple` comes with a `chainplot` utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53596623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpple.plot import chainplot\n",
    "\n",
    "chains = sampler.get_chain()\n",
    "chainplot(chains, labels=model.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e14fb",
   "metadata": {},
   "source": [
    "200 seems like a safe warm-up in this case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5357a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chains = sampler.get_chain(discard=200, flat=True, thin=5)\n",
    "corner.corner(\n",
    "    flat_chains,\n",
    "    labels=model.keys(),\n",
    "    truths=list(truths.values()),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec6ebc",
   "metadata": {},
   "source": [
    "To sample the posterior, we need to use the `get_posterior_pred()` function.\n",
    "It takes both the chains and a number of samples for arguments.\n",
    "`n_pred` samples will be drawn from the chain at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ad9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pred = 100\n",
    "ypreds = model.get_posterior_pred(flat_chains.T, n_pred, x)\n",
    "ax = plt.gca()\n",
    "for i, ypred in enumerate(ypreds):\n",
    "    ax.plot(x, ypred, \"C1-\", label=\"Posterior samples\" if i == 0 else None, alpha=0.1)\n",
    "ax.plot(x, y_true, label=\"True signal\")\n",
    "ax.errorbar(x, y, yerr=yerr, fmt=\"k.\", capsize=2, label=\"Simulated data\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_title(\"Posterior predictive samples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f141c",
   "metadata": {},
   "source": [
    "And we are done!\n",
    "\n",
    "This notebook shows all the basic steps to perform Bayesian inference with `simpple`.\n",
    "The main things that will change in real usage are the forward model and prior definitions (and of course the data).\n",
    "Everything else from this notebook should be re-usable almost as-is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
